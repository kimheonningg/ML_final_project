{"cells":[{"cell_type":"code","execution_count":42,"metadata":{"id":"18qUbGSROC1N","executionInfo":{"status":"ok","timestamp":1750595666038,"user_tz":-540,"elapsed":37,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d840a06c-36c8-425b-f913-f438dab30261"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrXoZsiYOC1N","executionInfo":{"status":"ok","timestamp":1750592943649,"user_tz":-540,"elapsed":51053,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"fd255f57-33d7-4492-b778-bf7cebafe145"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Running on Google Colab\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.13.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n","Collecting xgboost>=2.1.4 (from -r requirements.txt (line 6))\n","  Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n","Collecting torchmetrics (from -r requirements.txt (line 9))\n","  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n","Collecting tensorboard (from -r requirements.txt (line 12))\n","  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting tensorboardX (from -r requirements.txt (line 13))\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.58.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.2.3)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n","Collecting nvidia-nccl-cu12 (from xgboost>=2.1.4->-r requirements.txt (line 6))\n","  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics->-r requirements.txt (line 9)) (2.6.0+cpu)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics->-r requirements.txt (line 9))\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.73.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard->-r requirements.txt (line 12)) (3.3.6)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.17.0)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 12))\n","  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n","Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 12))\n","  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->-r requirements.txt (line 9)) (4.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (2025.5.1)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 12)) (3.0.2)\n","Downloading xgboost-3.0.2-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: werkzeug, tensorboardX, tensorboard-data-server, nvidia-nccl-cu12, lightning-utilities, xgboost, tensorboard, torchmetrics\n","Successfully installed lightning-utilities-0.14.3 nvidia-nccl-cu12-2.27.3 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorboardX-2.6.4 torchmetrics-1.7.3 werkzeug-3.1.3 xgboost-3.0.2\n"]}],"source":["import os\n","\n","\n","def is_colab():\n","    try:\n","        import google.colab\n","\n","        return True\n","    except ImportError:\n","        return False\n","\n","\n","COLAB_ROOT_PATH = \"/content/drive/MyDrive/final_project\"\n","\n","if is_colab():\n","    from google.colab import drive\n","    import sys\n","\n","    drive.mount(\"/content/drive\")\n","    if os.path.exists(COLAB_ROOT_PATH):\n","        os.chdir(COLAB_ROOT_PATH)\n","        sys.path.append(COLAB_ROOT_PATH)\n","    else:\n","        print(f\"{COLAB_ROOT_PATH} is not exist\")\n","\n","    print(\"Running on Google Colab\")\n","else:\n","    print(\"Not running on Google Colab\")\n","\n","\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"2xZQ9uErOC1O"},"source":["# Final Project Problem 2\n","\n","![Image](./image.png)\n","\n","In this jupyter notebook, we train a `neural network-based model` on the time series dataset from Problem 1.  \n","The conditions are as follows:\n","\n","- Must use the same feature engineering method as the final model in problem 1 (since our goal is to compare ensemble methods vs neural networks on the same dataset)\n","- Use pytorch freely to build the model, as used in assignments #3 and #4\n","- Specifically:\n","\n","  1. Use `problem_2.modeling.PandasDataset` to wrap the pandas dataset into a torch Dataset\n","  2. Build your model freely in `problem_2.modeling.load_model`\n","     - Input shape must match the features of final_train_dataset\n","     - Feel free to use anything including MLP or transformers\n","  3. Define optimizer and scheduler in `problem_2.modeling.load_optimizer` and `problem_2.modeling.load_scheduler`\n","     - Can use any optimizer defined in torch (ref: [link](https://docs.pytorch.org/docs/stable/optim.html#algorithms))\n","     - Can use any scheduler provided by torch (ref: [link](https://docs.pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate))\n","  4. Write training code in `problem_2.modeling.train`\n","     - Can write internal code freely but **must not modify the args**:\n","\n","  ```python\n","  def train(\n","    model: torch.nn.Module,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler,\n","    train_loader: torch.utils.data.DataLoader,\n","    val_loader: torch.utils.data.DataLoader,\n","    device: Optional[torch.device] = None,\n","    ) -> torch.nn.Module:\n","    ....\n","  ```\n","\n","  5. Model evaluation will be done using the `validate` function below\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"i8AkmuXPOC1O","executionInfo":{"status":"ok","timestamp":1750593003771,"user_tz":-540,"elapsed":57297,"user":{"displayName":"김희원","userId":"06298233953643740029"}}},"outputs":[],"source":["\"\"\"\n","The code below evaluates and saves the trained model.\n","Do not modify under any circumstances.\n","\"\"\"\n","import json\n","from typing import Callable, Optional, Tuple\n","import numpy as np\n","from torchmetrics.regression import MeanSquaredError\n","import torch\n","from pathlib import Path\n","\n","from problem_2.modeling import get_device\n","\n","\n","def validate(\n","    model: torch.nn.Module,\n","    val_loader: torch.utils.data.DataLoader,\n","    invese_y: Callable[[np.ndarray], np.ndarray],\n","    device: Optional[torch.device] = None,\n","):\n","    device = device or get_device()\n","    mse = MeanSquaredError()\n","    model.to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            x, y = batch\n","            x = x.to(device)\n","            y = y.to(device)\n","            pred_y = model(x)\n","\n","            invese_y_pred = torch.tensor(invese_y(pred_y.cpu().numpy().ravel()))\n","            invese_y_true = torch.tensor(invese_y(y.cpu().numpy().ravel()))\n","            mse.update(invese_y_pred, invese_y_true)\n","\n","    return mse.compute()\n","\n","\n","def save_model(\n","    model: torch.nn.Module,\n","    optim: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler,\n","    test_loader: torch.utils.data.DataLoader,\n","    train_loader: torch.utils.data.DataLoader,\n","    invese_y: Callable[[np.ndarray], np.ndarray],\n","    dst_path: str = \"problem_2_cache\",\n","    model_name: str = \"trained_model\",\n",") -> str:\n","    train_performance = validate(model, train_loader, invese_y)\n","    train_batch_size = train_loader.batch_size\n","    print(f\"train performance: {train_performance}\")\n","    test_performance = validate(model, test_loader, invese_y)\n","    test_batch_size = test_loader.batch_size\n","    print(f\"test performance: {test_performance}\")\n","\n","    dst_path = (\n","        Path(dst_path)\n","        / f\"{model_name}_tr_{train_performance:.4f}_te_{test_performance:.4f}_bs_{train_batch_size}_{test_batch_size}\"\n","    )\n","    if dst_path.exists():\n","        cnt_exist = len(list(dst_path.parent.glob(f\"{model_name}*\")))\n","        version = cnt_exist + 1\n","        dst_path = dst_path.parent / f\"{model_name}_{version}\"\n","    dst_path.mkdir(parents=True, exist_ok=True)\n","    print(f\"saving model to {dst_path}\")\n","\n","    torch.save(model.state_dict(), dst_path / \"model.pth\")\n","    torch.save(optim.state_dict(), dst_path / \"optim.pth\")\n","    torch.save(scheduler.state_dict(), dst_path / \"scheduler.pth\")\n","    with open(dst_path / \"performance.json\", \"w\") as f:\n","        json.dump(\n","            {\n","                \"tr_performance\": train_performance.item(),\n","                \"te_performance\": test_performance.item(),\n","            },\n","            f,\n","        )\n","    return dst_path\n","\n","\n","def load_trained_model(\n","    model_path: str,\n","    load_model: Callable[..., torch.nn.Module],\n","    load_optimizer: Callable[..., torch.optim.Optimizer],\n","    load_scheduler: Callable[..., torch.optim.lr_scheduler.LRScheduler],\n",") -> Tuple[\n","    torch.nn.Module,\n","    torch.optim.Optimizer,\n","    torch.optim.lr_scheduler.LRScheduler,\n","    float,\n","    float,\n","]:\n","    path_pth = Path(model_path)\n","    model_path = path_pth / \"model.pth\"\n","    optim_path = path_pth / \"optim.pth\"\n","    scheduler_path = path_pth / \"scheduler.pth\"\n","    performance_path = path_pth / \"performance.json\"\n","    with open(performance_path, \"r\") as f:\n","        performance = json.load(f)\n","    tr_performance = performance[\"tr_performance\"]\n","    te_performance = performance[\"te_performance\"]\n","    model = load_model()\n","    model.load_state_dict(torch.load(model_path, weights_only=True))\n","    optim = load_optimizer(model)\n","    optim.load_state_dict(torch.load(optim_path, weights_only=True))\n","    scheduler = load_scheduler(optim)\n","    scheduler.load_state_dict(torch.load(scheduler_path, weights_only=True))\n","    return model, optim, scheduler, tr_performance, te_performance\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZNVc2-ObOC1O","executionInfo":{"status":"ok","timestamp":1750593013386,"user_tz":-540,"elapsed":1470,"user":{"displayName":"김희원","userId":"06298233953643740029"}}},"outputs":[],"source":["from feature_engineering import final_feature_engineering\n","import pandas as pd\n","\n","training_dataset = pd.read_csv(\"ml_2025_final_project_training_dataset.csv\")\n","test_dataset = pd.read_csv(\"ml_2025_final_project_test_dataset.csv\")\n","\n","\n","final_train_dataset, final_test_dataset, final_inverse_y = final_feature_engineering(\n","    training_dataset, test_dataset\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rKO1izuUOC1P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750593016054,"user_tz":-540,"elapsed":81,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"83e7eb09-deec-468b-ee9a-692251d8e69d"},"outputs":[{"output_type":"stream","name":"stdout","text":["x.shape: torch.Size([256, 16])\n","y.shape: torch.Size([256])\n"]}],"source":["from problem_2.modeling import PandasDataset\n","from torch.utils.data import DataLoader\n","\n","train_batch_size = 256  # Feel free to modify\n","test_batch_size = 128  # Feel free to modify\n","\n","training_dataset = PandasDataset(final_train_dataset, \"y\")\n","test_dataset = PandasDataset(final_test_dataset, \"y\")\n","training_loader = DataLoader(\n","    training_dataset, batch_size=train_batch_size, shuffle=True\n",")\n","test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n","sample_batch = next(iter(training_loader))\n","x, y = sample_batch\n","print(f\"x.shape: {x.shape}\")\n","print(f\"y.shape: {y.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"lGp_ro8ROC1P"},"source":["## 1. Model Setup\n","\n","---\n","\n","- Please freely write your model in problem_2/modeling.py's `load_model`.\n","- Refer to sample_load_model and sampe_type_2_load_model in the same file.\n"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"RrN90WQlOC1P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750595675226,"user_tz":-540,"elapsed":514,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"592bc732-2fc3-4b17-ed42-daa2667559dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["TransformerModel(\n","  (encoder): Linear(in_features=16, out_features=128, bias=True)\n","  (pos_encoder): PositionalEncoding(\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","        (linear2): Linear(in_features=256, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","  )\n","  (decoder): Linear(in_features=128, out_features=1, bias=True)\n",")\n"]}],"source":["from problem_2.modeling import sample_load_model, load_model\n","\n","# example of loading model\n","# model = sample_load_model()\n","# please load your model here\n","model = load_model()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"5m2uSwcYOC1P"},"source":["## 2. Optimizer and Scheduler\n","\n","---\n","\n","- Please freely write your model in problem_2/modeling.py's `load_optimizer` and `load_optimizer`\n","- Refer to `sample_load_optimizer` and `sample_load_scheduler` in the same file.\n"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"SBdwVkReOC1P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750595677252,"user_tz":-540,"elapsed":39,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"d0ed9e25-c83d-412d-9687-5eb08e87730e"},"outputs":[{"output_type":"stream","name":"stdout","text":["AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    lr: 0.0001\n","    maximize: False\n","    weight_decay: 0.01\n",")\n","<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7b2bf56f8bd0>\n"]}],"source":["from problem_2.modeling import (\n","    sample_load_optimizer,\n","    sample_load_scheduler,\n","    load_optimizer,\n","    load_scheduler,\n",")\n","\n","# example of loading optimizer and scheduler\n","# optim = sample_load_optimizer(model=model)\n","# please load your optimizer here\n","optim = load_optimizer(model=model)\n","print(optim)\n","\n","# lr_scheduler = sample_load_scheduler(optimizer=optim)\n","# please load your scheduler here\n","lr_scheduler = load_scheduler(optimizer=optim)\n","print(lr_scheduler)"]},{"cell_type":"markdown","metadata":{"id":"enZFdZsWOC1P"},"source":["## 3. Training\n","\n","---\n","\n","- Please freely write your model in problem_2/modeling.py's `train`\n","- Refer to `sample_train` in the same file.\n"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"WeNXB4OLOC1Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"23420051-daaa-41bb-b935-b8e9feb8f108","executionInfo":{"status":"ok","timestamp":1750596107479,"user_tz":-540,"elapsed":427740,"user":{"displayName":"김희원","userId":"06298233953643740029"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Training Started ---\n","Epoch 1/30 | Train Loss: 836.2438 | Val Loss: 103.0011\n","  -> Found new best model with Val Loss: 103.0011\n","Epoch 2/30 | Train Loss: 660.1600 | Val Loss: 52.8996\n","  -> Found new best model with Val Loss: 52.8996\n","Epoch 3/30 | Train Loss: 517.5525 | Val Loss: 18.9980\n","  -> Found new best model with Val Loss: 18.9980\n","Epoch 4/30 | Train Loss: 384.6187 | Val Loss: 8.7572\n","  -> Found new best model with Val Loss: 8.7572\n","Epoch 5/30 | Train Loss: 269.6905 | Val Loss: 28.5186\n","  -> Val Loss did not improve. Patience: 1/7\n","Epoch 6/30 | Train Loss: 180.8297 | Val Loss: 83.4913\n","  -> Val Loss did not improve. Patience: 2/7\n","Epoch 7/30 | Train Loss: 123.7628 | Val Loss: 177.4922\n","  -> Val Loss did not improve. Patience: 3/7\n","Epoch 8/30 | Train Loss: 87.7178 | Val Loss: 38.7513\n","  -> Val Loss did not improve. Patience: 4/7\n","Epoch 9/30 | Train Loss: 58.1623 | Val Loss: 12.4453\n","  -> Val Loss did not improve. Patience: 5/7\n","Epoch 10/30 | Train Loss: 40.3952 | Val Loss: 8.1996\n","  -> Found new best model with Val Loss: 8.1996\n","Epoch 11/30 | Train Loss: 30.5871 | Val Loss: 5.1376\n","  -> Found new best model with Val Loss: 5.1376\n","Epoch 12/30 | Train Loss: 23.9580 | Val Loss: 4.5903\n","  -> Found new best model with Val Loss: 4.5903\n","Epoch 13/30 | Train Loss: 19.0639 | Val Loss: 4.8407\n","  -> Val Loss did not improve. Patience: 1/7\n","Epoch 14/30 | Train Loss: 15.1607 | Val Loss: 4.4082\n","  -> Found new best model with Val Loss: 4.4082\n","Epoch 15/30 | Train Loss: 11.7509 | Val Loss: 3.0572\n","  -> Found new best model with Val Loss: 3.0572\n","Epoch 16/30 | Train Loss: 9.5070 | Val Loss: 3.1701\n","  -> Val Loss did not improve. Patience: 1/7\n","Epoch 17/30 | Train Loss: 7.8613 | Val Loss: 2.3154\n","  -> Found new best model with Val Loss: 2.3154\n","Epoch 18/30 | Train Loss: 6.5370 | Val Loss: 2.0425\n","  -> Found new best model with Val Loss: 2.0425\n","Epoch 19/30 | Train Loss: 5.5331 | Val Loss: 2.1414\n","  -> Val Loss did not improve. Patience: 1/7\n","Epoch 20/30 | Train Loss: 4.8088 | Val Loss: 2.1250\n","  -> Val Loss did not improve. Patience: 2/7\n","Epoch 21/30 | Train Loss: 4.1558 | Val Loss: 1.9189\n","  -> Found new best model with Val Loss: 1.9189\n","Epoch 22/30 | Train Loss: 3.7914 | Val Loss: 1.8277\n","  -> Found new best model with Val Loss: 1.8277\n","Epoch 23/30 | Train Loss: 3.4358 | Val Loss: 1.6659\n","  -> Found new best model with Val Loss: 1.6659\n","Epoch 24/30 | Train Loss: 3.1161 | Val Loss: 2.0595\n","  -> Val Loss did not improve. Patience: 1/7\n","Epoch 25/30 | Train Loss: 2.9085 | Val Loss: 1.7779\n","  -> Val Loss did not improve. Patience: 2/7\n","Epoch 26/30 | Train Loss: 2.6766 | Val Loss: 1.4662\n","  -> Found new best model with Val Loss: 1.4662\n","Epoch 27/30 | Train Loss: 2.5133 | Val Loss: 1.3552\n","  -> Found new best model with Val Loss: 1.3552\n","Epoch 28/30 | Train Loss: 2.4098 | Val Loss: 1.2763\n","  -> Found new best model with Val Loss: 1.2763\n","Epoch 29/30 | Train Loss: 2.2599 | Val Loss: 1.3495\n","  -> Val Loss did not improve. Patience: 1/7\n","Epoch 30/30 | Train Loss: 2.1764 | Val Loss: 1.3836\n","  -> Val Loss did not improve. Patience: 2/7\n","\n","--- Loading best model from epoch with Val Loss: 1.2763 ---\n","--- Training Finished ---\n"]}],"source":["from problem_2.modeling import sample_train, train\n","\n","# example of training\n","# trained_model = sample_train(\n","#     model=model,\n","#     optimizer=optim,\n","#     scheduler=lr_scheduler,\n","#     train_loader=training_loader,\n","#     val_loader=test_loader,\n","# )\n","# please train your model here\n","trained_model = train(\n","    model=model,\n","    optimizer=optim,\n","    scheduler=lr_scheduler,\n","    train_loader=training_loader,\n","    val_loader=test_loader,\n",")"]},{"cell_type":"markdown","metadata":{"id":"8FGcLpF6OC1Q"},"source":["## 4. Validation\n","\n","---\n","\n","- Measure the final performance of your model.\n","- Note. The internal test dataset will also be measured in the same way.\n","\n","```python\n","validate(your_model, internal_test_loader, your_final_inverse_y)\n","```\n","\n","- Therefore, no errors should occur.\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"04DDKcjxOC1Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750596127914,"user_tz":-540,"elapsed":1016,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"f11fff26-e64b-4a68-e3e0-ca5af1bf8948"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.3023)"]},"metadata":{},"execution_count":47}],"source":["validate(model, test_loader, final_inverse_y)"]},{"cell_type":"markdown","metadata":{"id":"_OBDD9F3OC1Q"},"source":["## 5. Save Trained Model\n","\n","---\n","\n","- Save your model and submit it\n","\n","- This code is used to test your code as it is called and tested during grading, so it must work and no modifications to the code are allowed.\n"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"SCJ09W-9OC1Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750596139074,"user_tz":-540,"elapsed":9644,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"fdf87f02-8493-43c7-a1a6-32be403e3727"},"outputs":[{"output_type":"stream","name":"stdout","text":["train performance: 0.9884642362594604\n","test performance: 1.3023041486740112\n","saving model to problem_2_cache/trained_model_tr_0.9885_te_1.3023_bs_256_128\n"]}],"source":["save_path = save_model(\n","    model=model,\n","    optim=optim,\n","    scheduler=lr_scheduler,\n","    train_loader=training_loader,\n","    test_loader=test_loader,\n","    invese_y=final_inverse_y,\n",")"]},{"cell_type":"markdown","metadata":{"id":"-c6tdvvZOC1Q"},"source":["## 6. Load trained Model\n","\n","---\n","\n","Load your saved model\n"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"eiLJ_IDyOC1Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750596333140,"user_tz":-540,"elapsed":94,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"afae6fde-3751-4a44-ee89-50bf1978e796"},"outputs":[{"output_type":"stream","name":"stdout","text":["TransformerModel(\n","  (encoder): Linear(in_features=16, out_features=128, bias=True)\n","  (pos_encoder): PositionalEncoding(\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-2): 3 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","        (linear2): Linear(in_features=256, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","  )\n","  (decoder): Linear(in_features=128, out_features=1, bias=True)\n",")\n","AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    initial_lr: 0.0001\n","    lr: 3.5203658778440106e-05\n","    maximize: False\n","    weight_decay: 0.01\n",")\n","<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7b2be30a6e50>\n","0.9884642362594604\n","1.3023041486740112\n"]}],"source":["model, optim, scheduler, tr_performance, te_performance = load_trained_model(\n","    model_path=save_path,\n","    load_model=load_model,\n","    load_optimizer=load_optimizer,\n","    load_scheduler=load_scheduler,\n",")\n","print(model)\n","print(optim)\n","print(scheduler)\n","print(tr_performance)\n","print(te_performance)"]},{"cell_type":"markdown","metadata":{"id":"FI8lxdGWOC1Q"},"source":["### Problem Summary\n","\n","- Final submission file structure\n","- problem_2 /\n","  - best_model\n","    - model.pth\n","    - optim.pth\n","    - performance.json\n","    - scheduler.pth\n","- modeling.py\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}