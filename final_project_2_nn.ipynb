{"cells":[{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1750695273587,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"18qUbGSROC1N","outputId":"500d60b2-be30-4368-8247-ce9175f93659"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7101,"status":"ok","timestamp":1750695256252,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"BrXoZsiYOC1N","outputId":"675fb874-5502-4209-b795-4fb13ef67ec7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Running on Google Colab\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.13.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n","Requirement already satisfied: xgboost>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.1.4)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.7.3)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.18.0)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (2.6.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.58.4)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.2.3)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost>=2.1.4->-r requirements.txt (line 6)) (2.21.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics->-r requirements.txt (line 9)) (2.6.0+cu124)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics->-r requirements.txt (line 9)) (0.14.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.73.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (3.8)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 12)) (3.1.3)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->-r requirements.txt (line 9)) (4.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (0.6.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics->-r requirements.txt (line 9)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 12)) (3.0.2)\n"]}],"source":["import os\n","\n","\n","def is_colab():\n","    try:\n","        import google.colab\n","\n","        return True\n","    except ImportError:\n","        return False\n","\n","\n","COLAB_ROOT_PATH = \"/content/drive/MyDrive/final_project\"\n","\n","if is_colab():\n","    from google.colab import drive\n","    import sys\n","\n","    drive.mount(\"/content/drive\")\n","    if os.path.exists(COLAB_ROOT_PATH):\n","        os.chdir(COLAB_ROOT_PATH)\n","        sys.path.append(COLAB_ROOT_PATH)\n","    else:\n","        print(f\"{COLAB_ROOT_PATH} is not exist\")\n","\n","    print(\"Running on Google Colab\")\n","else:\n","    print(\"Not running on Google Colab\")\n","\n","\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"2xZQ9uErOC1O"},"source":["# Final Project Problem 2\n","\n","![Image](./image.png)\n","\n","In this jupyter notebook, we train a `neural network-based model` on the time series dataset from Problem 1.  \n","The conditions are as follows:\n","\n","- Must use the same feature engineering method as the final model in problem 1 (since our goal is to compare ensemble methods vs neural networks on the same dataset)\n","- Use pytorch freely to build the model, as used in assignments #3 and #4\n","- Specifically:\n","\n","  1. Use `problem_2.modeling.PandasDataset` to wrap the pandas dataset into a torch Dataset\n","  2. Build your model freely in `problem_2.modeling.load_model`\n","     - Input shape must match the features of final_train_dataset\n","     - Feel free to use anything including MLP or transformers\n","  3. Define optimizer and scheduler in `problem_2.modeling.load_optimizer` and `problem_2.modeling.load_scheduler`\n","     - Can use any optimizer defined in torch (ref: [link](https://docs.pytorch.org/docs/stable/optim.html#algorithms))\n","     - Can use any scheduler provided by torch (ref: [link](https://docs.pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate))\n","  4. Write training code in `problem_2.modeling.train`\n","     - Can write internal code freely but **must not modify the args**:\n","\n","  ```python\n","  def train(\n","    model: torch.nn.Module,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler,\n","    train_loader: torch.utils.data.DataLoader,\n","    val_loader: torch.utils.data.DataLoader,\n","    device: Optional[torch.device] = None,\n","    ) -> torch.nn.Module:\n","    ....\n","  ```\n","\n","  5. Model evaluation will be done using the `validate` function below\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":13678,"status":"ok","timestamp":1750695145803,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"i8AkmuXPOC1O"},"outputs":[],"source":["\"\"\"\n","The code below evaluates and saves the trained model.\n","Do not modify under any circumstances.\n","\"\"\"\n","import json\n","from typing import Callable, Optional, Tuple\n","import numpy as np\n","from torchmetrics.regression import MeanSquaredError\n","import torch\n","from pathlib import Path\n","\n","from problem_2.modeling import get_device\n","\n","\n","def validate(\n","    model: torch.nn.Module,\n","    val_loader: torch.utils.data.DataLoader,\n","    invese_y: Callable[[np.ndarray], np.ndarray],\n","    device: Optional[torch.device] = None,\n","):\n","    device = device or get_device()\n","    mse = MeanSquaredError()\n","    model.to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            x, y = batch\n","            x = x.to(device)\n","            y = y.to(device)\n","            pred_y = model(x)\n","\n","            invese_y_pred = torch.tensor(invese_y(pred_y.cpu().numpy().ravel()))\n","            invese_y_true = torch.tensor(invese_y(y.cpu().numpy().ravel()))\n","            mse.update(invese_y_pred, invese_y_true)\n","\n","    return mse.compute()\n","\n","\n","def save_model(\n","    model: torch.nn.Module,\n","    optim: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LRScheduler,\n","    test_loader: torch.utils.data.DataLoader,\n","    train_loader: torch.utils.data.DataLoader,\n","    invese_y: Callable[[np.ndarray], np.ndarray],\n","    dst_path: str = \"problem_2_cache\",\n","    model_name: str = \"trained_model\",\n",") -> str:\n","    train_performance = validate(model, train_loader, invese_y)\n","    train_batch_size = train_loader.batch_size\n","    print(f\"train performance: {train_performance}\")\n","    test_performance = validate(model, test_loader, invese_y)\n","    test_batch_size = test_loader.batch_size\n","    print(f\"test performance: {test_performance}\")\n","\n","    dst_path = (\n","        Path(dst_path)\n","        / f\"{model_name}_tr_{train_performance:.4f}_te_{test_performance:.4f}_bs_{train_batch_size}_{test_batch_size}\"\n","    )\n","    if dst_path.exists():\n","        cnt_exist = len(list(dst_path.parent.glob(f\"{model_name}*\")))\n","        version = cnt_exist + 1\n","        dst_path = dst_path.parent / f\"{model_name}_{version}\"\n","    dst_path.mkdir(parents=True, exist_ok=True)\n","    print(f\"saving model to {dst_path}\")\n","\n","    torch.save(model.state_dict(), dst_path / \"model.pth\")\n","    torch.save(optim.state_dict(), dst_path / \"optim.pth\")\n","    torch.save(scheduler.state_dict(), dst_path / \"scheduler.pth\")\n","    with open(dst_path / \"performance.json\", \"w\") as f:\n","        json.dump(\n","            {\n","                \"tr_performance\": train_performance.item(),\n","                \"te_performance\": test_performance.item(),\n","            },\n","            f,\n","        )\n","    return dst_path\n","\n","\n","def load_trained_model(\n","    model_path: str,\n","    load_model: Callable[..., torch.nn.Module],\n","    load_optimizer: Callable[..., torch.optim.Optimizer],\n","    load_scheduler: Callable[..., torch.optim.lr_scheduler.LRScheduler],\n",") -> Tuple[\n","    torch.nn.Module,\n","    torch.optim.Optimizer,\n","    torch.optim.lr_scheduler.LRScheduler,\n","    float,\n","    float,\n","]:\n","    path_pth = Path(model_path)\n","    model_path = path_pth / \"model.pth\"\n","    optim_path = path_pth / \"optim.pth\"\n","    scheduler_path = path_pth / \"scheduler.pth\"\n","    performance_path = path_pth / \"performance.json\"\n","    with open(performance_path, \"r\") as f:\n","        performance = json.load(f)\n","    tr_performance = performance[\"tr_performance\"]\n","    te_performance = performance[\"te_performance\"]\n","    model = load_model()\n","    model.load_state_dict(torch.load(model_path, weights_only=True))\n","    optim = load_optimizer(model)\n","    optim.load_state_dict(torch.load(optim_path, weights_only=True))\n","    scheduler = load_scheduler(optim)\n","    scheduler.load_state_dict(torch.load(scheduler_path, weights_only=True))\n","    return model, optim, scheduler, tr_performance, te_performance\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1738,"status":"ok","timestamp":1750695150145,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"ZNVc2-ObOC1O"},"outputs":[],"source":["from feature_engineering import final_feature_engineering\n","import pandas as pd\n","\n","training_dataset = pd.read_csv(\"ml_2025_final_project_training_dataset.csv\")\n","test_dataset = pd.read_csv(\"ml_2025_final_project_test_dataset.csv\")\n","\n","\n","final_train_dataset, final_test_dataset, final_inverse_y = final_feature_engineering(\n","    training_dataset, test_dataset\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1750695151351,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"rKO1izuUOC1P","outputId":"20b91b4e-6c7b-4508-a56e-0d9644a5d8ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["x.shape: torch.Size([128, 15])\n","y.shape: torch.Size([128])\n"]}],"source":["from problem_2.modeling import PandasDataset\n","from torch.utils.data import DataLoader\n","\n","train_batch_size = 128  # Feel free to modify\n","test_batch_size = 128  # Feel free to modify\n","\n","training_dataset = PandasDataset(final_train_dataset, \"y\")\n","test_dataset = PandasDataset(final_test_dataset, \"y\")\n","training_loader = DataLoader(\n","    training_dataset, batch_size=train_batch_size, shuffle=True\n",")\n","test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n","sample_batch = next(iter(training_loader))\n","x, y = sample_batch\n","print(f\"x.shape: {x.shape}\")\n","print(f\"y.shape: {y.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"lGp_ro8ROC1P"},"source":["## 1. Model Setup\n","\n","---\n","\n","- Please freely write your model in problem_2/modeling.py's `load_model`.\n","- Refer to sample_load_model and sampe_type_2_load_model in the same file.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1750695282818,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"RrN90WQlOC1P","outputId":"efcc9ec5-c789-4b72-e1ff-4ad0be6b06ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["ETSformerWrapper(\n","  (actual_model): ETSformer(\n","    (embedding): Linear(in_features=15, out_features=64, bias=True)\n","    (layers): ModuleList(\n","      (0-1): 2 x ETSLayer(\n","        (smooth): Sequential(\n","          (0): Linear(in_features=64, out_features=64, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=64, out_features=64, bias=True)\n","        )\n","        (growth): Linear(in_features=64, out_features=64, bias=True)\n","        (fourier): FourierLayer()\n","        (seasonal): Linear(in_features=20, out_features=64, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (decoder): Linear(in_features=6400, out_features=1, bias=True)\n","  )\n",")\n"]}],"source":["from problem_2.modeling import sample_load_model, load_model\n","\n","# example of loading model\n","# model = sample_load_model()\n","# please load your model here\n","model = load_model()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"5m2uSwcYOC1P"},"source":["## 2. Optimizer and Scheduler\n","\n","---\n","\n","- Please freely write your model in problem_2/modeling.py's `load_optimizer` and `load_optimizer`\n","- Refer to `sample_load_optimizer` and `sample_load_scheduler` in the same file.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1750695284512,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"SBdwVkReOC1P","outputId":"6db7c197-e5d0-4212-8031-087fc04e03e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    lr: 0.001\n","    maximize: False\n","    weight_decay: 0.01\n",")\n","<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f7d04977cd0>\n"]}],"source":["from problem_2.modeling import (\n","    sample_load_optimizer,\n","    sample_load_scheduler,\n","    load_optimizer,\n","    load_scheduler,\n",")\n","\n","# example of loading optimizer and scheduler\n","# optim = sample_load_optimizer(model=model)\n","# please load your optimizer here\n","optim = load_optimizer(model=model)\n","print(optim)\n","\n","# lr_scheduler = sample_load_scheduler(optimizer=optim)\n","# please load your scheduler here\n","lr_scheduler = load_scheduler(optimizer=optim)\n","print(lr_scheduler)"]},{"cell_type":"markdown","metadata":{"id":"enZFdZsWOC1P"},"source":["## 3. Training\n","\n","---\n","\n","- Please freely write your model in problem_2/modeling.py's `train`\n","- Refer to `sample_train` in the same file.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WeNXB4OLOC1Q","executionInfo":{"status":"ok","timestamp":1750695790791,"user_tz":-540,"elapsed":504593,"user":{"displayName":"김희원","userId":"06298233953643740029"}},"outputId":"39b21dc7-78fd-4abd-86bc-609cdc40b5e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Training Started ---\n","  Epoch 1/20 | Processing batch 50/391\n","  Epoch 1/20 | Processing batch 100/391\n","  Epoch 1/20 | Processing batch 150/391\n","  Epoch 1/20 | Processing batch 200/391\n","  Epoch 1/20 | Processing batch 250/391\n","  Epoch 1/20 | Processing batch 300/391\n","  Epoch 1/20 | Processing batch 350/391\n","Epoch 1/20 | Train Loss: 26.3255 | Val Loss: 0.8439\n","  -> Found new best model with Val Loss: 0.8439\n","  Epoch 2/20 | Processing batch 50/391\n","  Epoch 2/20 | Processing batch 100/391\n","  Epoch 2/20 | Processing batch 150/391\n","  Epoch 2/20 | Processing batch 200/391\n","  Epoch 2/20 | Processing batch 250/391\n","  Epoch 2/20 | Processing batch 300/391\n","  Epoch 2/20 | Processing batch 350/391\n","Epoch 2/20 | Train Loss: 4.1235 | Val Loss: 2.0787\n","  Epoch 3/20 | Processing batch 50/391\n","  Epoch 3/20 | Processing batch 100/391\n","  Epoch 3/20 | Processing batch 150/391\n","  Epoch 3/20 | Processing batch 200/391\n","  Epoch 3/20 | Processing batch 250/391\n","  Epoch 3/20 | Processing batch 300/391\n","  Epoch 3/20 | Processing batch 350/391\n","Epoch 3/20 | Train Loss: 2.5034 | Val Loss: 2.0395\n","  Epoch 4/20 | Processing batch 50/391\n","  Epoch 4/20 | Processing batch 100/391\n","  Epoch 4/20 | Processing batch 150/391\n","  Epoch 4/20 | Processing batch 200/391\n","  Epoch 4/20 | Processing batch 250/391\n","  Epoch 4/20 | Processing batch 300/391\n","  Epoch 4/20 | Processing batch 350/391\n","Epoch 4/20 | Train Loss: 1.7304 | Val Loss: 0.2447\n","  -> Found new best model with Val Loss: 0.2447\n","  Epoch 5/20 | Processing batch 50/391\n","  Epoch 5/20 | Processing batch 100/391\n","  Epoch 5/20 | Processing batch 150/391\n","  Epoch 5/20 | Processing batch 200/391\n","  Epoch 5/20 | Processing batch 250/391\n","  Epoch 5/20 | Processing batch 300/391\n","  Epoch 5/20 | Processing batch 350/391\n","Epoch 5/20 | Train Loss: 1.4919 | Val Loss: 2.3828\n","  Epoch 6/20 | Processing batch 50/391\n","  Epoch 6/20 | Processing batch 100/391\n","  Epoch 6/20 | Processing batch 150/391\n","  Epoch 6/20 | Processing batch 200/391\n","  Epoch 6/20 | Processing batch 250/391\n","  Epoch 6/20 | Processing batch 300/391\n","  Epoch 6/20 | Processing batch 350/391\n","Epoch 6/20 | Train Loss: 1.7054 | Val Loss: 0.9223\n","  Epoch 7/20 | Processing batch 50/391\n","  Epoch 7/20 | Processing batch 100/391\n","  Epoch 7/20 | Processing batch 150/391\n","  Epoch 7/20 | Processing batch 200/391\n","  Epoch 7/20 | Processing batch 250/391\n","  Epoch 7/20 | Processing batch 300/391\n","  Epoch 7/20 | Processing batch 350/391\n","Epoch 7/20 | Train Loss: 1.5365 | Val Loss: 0.2895\n","  Epoch 8/20 | Processing batch 50/391\n","  Epoch 8/20 | Processing batch 100/391\n","  Epoch 8/20 | Processing batch 150/391\n","  Epoch 8/20 | Processing batch 200/391\n","  Epoch 8/20 | Processing batch 250/391\n","  Epoch 8/20 | Processing batch 300/391\n","  Epoch 8/20 | Processing batch 350/391\n","Epoch 8/20 | Train Loss: 0.9994 | Val Loss: 0.3960\n","  Epoch 9/20 | Processing batch 50/391\n","  Epoch 9/20 | Processing batch 100/391\n","  Epoch 9/20 | Processing batch 150/391\n","  Epoch 9/20 | Processing batch 200/391\n","  Epoch 9/20 | Processing batch 250/391\n","  Epoch 9/20 | Processing batch 300/391\n","  Epoch 9/20 | Processing batch 350/391\n","Epoch 9/20 | Train Loss: 0.9679 | Val Loss: 0.2862\n","  Epoch 10/20 | Processing batch 50/391\n","  Epoch 10/20 | Processing batch 100/391\n","  Epoch 10/20 | Processing batch 150/391\n","  Epoch 10/20 | Processing batch 200/391\n","  Epoch 10/20 | Processing batch 250/391\n","  Epoch 10/20 | Processing batch 300/391\n","  Epoch 10/20 | Processing batch 350/391\n","Epoch 10/20 | Train Loss: 0.8745 | Val Loss: 0.1816\n","  -> Found new best model with Val Loss: 0.1816\n","  Epoch 11/20 | Processing batch 50/391\n","  Epoch 11/20 | Processing batch 100/391\n","  Epoch 11/20 | Processing batch 150/391\n","  Epoch 11/20 | Processing batch 200/391\n","  Epoch 11/20 | Processing batch 250/391\n","  Epoch 11/20 | Processing batch 300/391\n","  Epoch 11/20 | Processing batch 350/391\n","Epoch 11/20 | Train Loss: 0.9387 | Val Loss: 0.2683\n","  Epoch 12/20 | Processing batch 50/391\n","  Epoch 12/20 | Processing batch 100/391\n","  Epoch 12/20 | Processing batch 150/391\n","  Epoch 12/20 | Processing batch 200/391\n","  Epoch 12/20 | Processing batch 250/391\n","  Epoch 12/20 | Processing batch 300/391\n","  Epoch 12/20 | Processing batch 350/391\n","Epoch 12/20 | Train Loss: 0.6611 | Val Loss: 0.9010\n","  Epoch 13/20 | Processing batch 50/391\n","  Epoch 13/20 | Processing batch 100/391\n","  Epoch 13/20 | Processing batch 150/391\n","  Epoch 13/20 | Processing batch 200/391\n","  Epoch 13/20 | Processing batch 250/391\n","  Epoch 13/20 | Processing batch 300/391\n","  Epoch 13/20 | Processing batch 350/391\n","Epoch 13/20 | Train Loss: 0.6733 | Val Loss: 1.0198\n","  Epoch 14/20 | Processing batch 50/391\n","  Epoch 14/20 | Processing batch 100/391\n","  Epoch 14/20 | Processing batch 150/391\n","  Epoch 14/20 | Processing batch 200/391\n","  Epoch 14/20 | Processing batch 250/391\n","  Epoch 14/20 | Processing batch 300/391\n","  Epoch 14/20 | Processing batch 350/391\n","Epoch 14/20 | Train Loss: 0.5875 | Val Loss: 0.7484\n","  Epoch 15/20 | Processing batch 50/391\n","  Epoch 15/20 | Processing batch 100/391\n","  Epoch 15/20 | Processing batch 150/391\n","  Epoch 15/20 | Processing batch 200/391\n","  Epoch 15/20 | Processing batch 250/391\n","  Epoch 15/20 | Processing batch 300/391\n","  Epoch 15/20 | Processing batch 350/391\n","Epoch 15/20 | Train Loss: 0.5600 | Val Loss: 0.5582\n","  Epoch 16/20 | Processing batch 50/391\n","  Epoch 16/20 | Processing batch 100/391\n","  Epoch 16/20 | Processing batch 150/391\n","  Epoch 16/20 | Processing batch 200/391\n","  Epoch 16/20 | Processing batch 250/391\n","  Epoch 16/20 | Processing batch 300/391\n","  Epoch 16/20 | Processing batch 350/391\n","Epoch 16/20 | Train Loss: 0.4958 | Val Loss: 0.7640\n","  Epoch 17/20 | Processing batch 50/391\n","  Epoch 17/20 | Processing batch 100/391\n","  Epoch 17/20 | Processing batch 150/391\n","  Epoch 17/20 | Processing batch 200/391\n","  Epoch 17/20 | Processing batch 250/391\n","  Epoch 17/20 | Processing batch 300/391\n","  Epoch 17/20 | Processing batch 350/391\n","Epoch 17/20 | Train Loss: 0.4897 | Val Loss: 0.6223\n","  Epoch 18/20 | Processing batch 50/391\n","  Epoch 18/20 | Processing batch 100/391\n","  Epoch 18/20 | Processing batch 150/391\n","  Epoch 18/20 | Processing batch 200/391\n","  Epoch 18/20 | Processing batch 250/391\n","  Epoch 18/20 | Processing batch 300/391\n","  Epoch 18/20 | Processing batch 350/391\n","Epoch 18/20 | Train Loss: 0.4528 | Val Loss: 0.8170\n","  Epoch 19/20 | Processing batch 50/391\n","  Epoch 19/20 | Processing batch 100/391\n","  Epoch 19/20 | Processing batch 150/391\n","  Epoch 19/20 | Processing batch 200/391\n","  Epoch 19/20 | Processing batch 250/391\n","  Epoch 19/20 | Processing batch 300/391\n","  Epoch 19/20 | Processing batch 350/391\n","Epoch 19/20 | Train Loss: 0.4324 | Val Loss: 0.9825\n","  Epoch 20/20 | Processing batch 50/391\n","  Epoch 20/20 | Processing batch 100/391\n","  Epoch 20/20 | Processing batch 150/391\n","  Epoch 20/20 | Processing batch 200/391\n","  Epoch 20/20 | Processing batch 250/391\n","  Epoch 20/20 | Processing batch 300/391\n","  Epoch 20/20 | Processing batch 350/391\n","Epoch 20/20 | Train Loss: 0.4253 | Val Loss: 1.0859\n","\n","--- Loading best model from epoch with Val Loss: 0.1816 ---\n","--- Training Finished ---\n"]}],"source":["from problem_2.modeling import sample_train, train\n","\n","# example of training\n","# trained_model = sample_train(\n","#     model=model,\n","#     optimizer=optim,\n","#     scheduler=lr_scheduler,\n","#     train_loader=training_loader,\n","#     val_loader=test_loader,\n","# )\n","# please train your model here\n","trained_model = train(\n","    model=model,\n","    optimizer=optim,\n","    scheduler=lr_scheduler,\n","    train_loader=training_loader,\n","    val_loader=test_loader,\n",")"]},{"cell_type":"markdown","metadata":{"id":"8FGcLpF6OC1Q"},"source":["## 4. Validation\n","\n","---\n","\n","- Measure the final performance of your model.\n","- Note. The internal test dataset will also be measured in the same way.\n","\n","```python\n","validate(your_model, internal_test_loader, your_final_inverse_y)\n","```\n","\n","- Therefore, no errors should occur.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2187,"status":"ok","timestamp":1750696273053,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"04DDKcjxOC1Q","outputId":"395741e1-00bb-4f98-daa0-0f3080abf33c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.1844)"]},"metadata":{},"execution_count":22}],"source":["validate(model, test_loader, final_inverse_y)"]},{"cell_type":"markdown","metadata":{"id":"_OBDD9F3OC1Q"},"source":["## 5. Save Trained Model\n","\n","---\n","\n","- Save your model and submit it\n","\n","- This code is used to test your code as it is called and tested during grading, so it must work and no modifications to the code are allowed.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23605,"status":"ok","timestamp":1750696298002,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"SCJ09W-9OC1Q","outputId":"44f18396-33e4-4822-a9bd-31972acd3785"},"outputs":[{"output_type":"stream","name":"stdout","text":["train performance: 0.7449092268943787\n","test performance: 0.18439459800720215\n","saving model to problem_2_cache/trained_model_tr_0.7449_te_0.1844_bs_128_128\n"]}],"source":["save_path = save_model(\n","    model=model,\n","    optim=optim,\n","    scheduler=lr_scheduler,\n","    train_loader=training_loader,\n","    test_loader=test_loader,\n","    invese_y=final_inverse_y,\n",")"]},{"cell_type":"markdown","metadata":{"id":"-c6tdvvZOC1Q"},"source":["## 6. Load trained Model\n","\n","---\n","\n","Load your saved model\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64,"status":"ok","timestamp":1750696300309,"user":{"displayName":"김희원","userId":"06298233953643740029"},"user_tz":-540},"id":"eiLJ_IDyOC1Q","outputId":"f60f3418-9f3f-43b1-b573-43a0a951a027"},"outputs":[{"output_type":"stream","name":"stdout","text":["ETSformerWrapper(\n","  (actual_model): ETSformer(\n","    (embedding): Linear(in_features=15, out_features=64, bias=True)\n","    (layers): ModuleList(\n","      (0-1): 2 x ETSLayer(\n","        (smooth): Sequential(\n","          (0): Linear(in_features=64, out_features=64, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=64, out_features=64, bias=True)\n","        )\n","        (growth): Linear(in_features=64, out_features=64, bias=True)\n","        (fourier): FourierLayer()\n","        (seasonal): Linear(in_features=20, out_features=64, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","    (decoder): Linear(in_features=6400, out_features=1, bias=True)\n","  )\n",")\n","AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    initial_lr: 0.001\n","    lr: 0.0001\n","    maximize: False\n","    weight_decay: 0.01\n",")\n","<torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f7cf90c9510>\n","0.7449092268943787\n","0.18439459800720215\n"]}],"source":["model, optim, scheduler, tr_performance, te_performance = load_trained_model(\n","    model_path=save_path,\n","    load_model=load_model,\n","    load_optimizer=load_optimizer,\n","    load_scheduler=load_scheduler,\n",")\n","print(model)\n","print(optim)\n","print(scheduler)\n","print(tr_performance)\n","print(te_performance)"]},{"cell_type":"markdown","metadata":{"id":"FI8lxdGWOC1Q"},"source":["### Problem Summary\n","\n","- Final submission file structure\n","- problem_2 /\n","  - best_model\n","    - model.pth\n","    - optim.pth\n","    - performance.json\n","    - scheduler.pth\n","- modeling.py\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}